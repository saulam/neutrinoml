{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "colab": {
      "name": "regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVjl-kju6o4S"
      },
      "source": [
        "# Momentum regression\n",
        "\n",
        "This assignment aims to learn how to define and run a regression model for reconstructing the momentum of neutrino events. Regression models are typical in machine learning, where the algorithm learns how to estimate a **continuous value** from a vector of features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UG5VHLK7-iqp"
      },
      "source": [
        "##Prerequisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHMkB-apwSJy"
      },
      "source": [
        "Let's start with downloading the dataset, as well as loading the needed Python packages and modules:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1HmRISt6o4V"
      },
      "source": [
        "!wget \"https://raw.githubusercontent.com/saulam/neutrinoml/main/modules.py\"\n",
        "!wget \"https://raw.githubusercontent.com/saulam/neutrinoml/main/df_pgun_teaching.p\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import scale, PolynomialFeatures\n",
        "from sklearn.model_selection import train_test_split\n",
        "from modules import plot_event, plot_regression, plot_parameters, plot_distributions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVHwIwSOvz3n"
      },
      "source": [
        "##Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sc7lPCGT6o4W"
      },
      "source": [
        "We can now load the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEgXr0CV6o4W"
      },
      "source": [
        "# read dataframe\n",
        "df = pd.read_pickle('df_pgun_teaching.p')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSBVlOV-6o4W"
      },
      "source": [
        "We may have a look at the dataset. It consists of 59,578 particle gun events with the following attributes:\n",
        "\n",
        "- **TruePID**: PDG code for particle identification (PID); 2212 (proton), 13 (muon), 211 (pion).\n",
        "- **TrueMomentum**: momentum in MeV.\n",
        "- **NNodes**: number of nodes of the event (3D spatial points).\n",
        "- **NodeOrder**: order of the nodes within the event.\n",
        "- **NodePosX**: array with the coordinates of the nodes along the X-axis (in mm).\n",
        "- **NodePosY**: array with the coordinates of the nodes along the Y-axis (in mm).\n",
        "- **NodePosZ**: array with the coordinates of the nodes along the Z-axis (in mm).\n",
        "- **NodeT**: array with the timestamps of the nodes (in ms).\n",
        "- **Nodededx**: array with energy deposits of the nodes (dE/dx).\n",
        "- **TrkLen**: length of the track (in mm).\n",
        "- **TrkEDepo**: total track energy deposition (in arbitrary unit).\n",
        "- **TrkDir1**: track direction, polar angle (in degrees).\n",
        "- **TrkDir2**: track direction, azimuth angle (in degrees).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9FuASKM6o4X"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrzNwzscqd4w"
      },
      "source": [
        "The 3D spatial points of the events are usually stored in the form of hits or nodes. We chose the latter for our dataset. A hit corresponds with a cube with real energy deposition (there are usually many hits across the track signature), whilst a node corresponds with a fitted position after performing the track reconstruction.\n",
        "\n",
        "<div>\n",
        "<img src=\"https://raw.githubusercontent.com/saulam/neutrinoml/main/hit.png\" height=\"400\"/>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
        "<img src=\"https://raw.githubusercontent.com/saulam/neutrinoml/main/node.png\" height=\"400\"/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5qxEri_6o4X"
      },
      "source": [
        "We may also have a look at the events by plotting the nodes within the detector space. By default, we're looking at the first event (event 0), but we can display more events by playing with the variable `event_number`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8c9eBj3T6o4Y"
      },
      "source": [
        "event_number = 0\n",
        "plot_event(df, event_number)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g70lBcA36o4Z"
      },
      "source": [
        "Regardless of the type of data we use and the algorithm chosen, it is essential to perform a **preprocessing** of the data, which allows us to prepare the data to make it understandable for the machine-learning algorithm.\n",
        "\n",
        "As explained before, the goal is to learn to estimate continuous value **y** from a fixed-size vector of features **X**. However, the input data is in 3D, and every event (track) has a different size. Thus, the simplest way of preprocessing our data is to use only the dE/dx of the first node of each track as input:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yfqZYjY6o4Z"
      },
      "source": [
        "X = np.zeros(shape=(len(df),1), dtype=np.float32) # array of size (n_event, 1)\n",
        "y = np.zeros(shape=(len(df),), dtype=np.float32)  # array of size (n_event,)\n",
        "\n",
        "# fill dataset\n",
        "for event_n, event in df.iterrows():\n",
        "    \n",
        "    NodeOrder = event['NodeOrder']\n",
        "    Nodededx = event['Nodededx'][NodeOrder]\n",
        "    \n",
        "    # retrieve the first node\n",
        "    X[event_n] = Nodededx[0]\n",
        "    # momentum label\n",
        "    y[event_n] = event['TrueMomentum']\n",
        "\n",
        "# standardize the dataset (mean=0, std=1)\n",
        "X_stan = scale(X) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vlvQHUc6o4Z"
      },
      "source": [
        "In order to understand the training data, it's always good to visualise first. A good way of doing it could be creating a scatter plot of each of our variables against the momentum:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDQfji9m6o4a"
      },
      "source": [
        "param_names = ['momentum (MeV)', 'dE/dx node 1']\n",
        "plot_parameters(X, y, param_names, y_names=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zk-rUJT36o4b"
      },
      "source": [
        "Unfortunately, it doesn't look like the input follows a linear distribution. Anyway, we could train a linear regression model on the input and see what it learns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BX5jfc3Q6o4b"
      },
      "source": [
        "Training a machine-learning algorithm is usually not an easy task. The algorithm learns from some training data until it is ready to make predictions on unseen data. In order to test how the algorithm performs on new data, the datasets used for training are divided into two groups (sometimes are divided into three groups, but we're keeping two groups here for simplicity):\n",
        "\n",
        "- Training set: the model learns from this set only. It must be the largest set.\n",
        "- Test set: it is used to evaluate the model at the end of the training, only once it is fully trained. \n",
        "\n",
        "In this example, we keep 60% of the data for training and 40% for testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74sobEvb6o4c"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_stan, y, test_size=0.4, random_state=7) # 60% training and 40% test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ypfok2Nfv_Jj"
      },
      "source": [
        "##Linear regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixC4mpcR6o4d"
      },
      "source": [
        "Now we can run the linear regression on the training set and print the coefficients learnt:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWa9G6aF6o4d"
      },
      "source": [
        "reg = LinearRegression().fit(X_train, y_train)\n",
        "print(\"B0: {}, B1: {}\".format(reg.intercept_, reg.coef_[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEEYbeJU6o4e"
      },
      "source": [
        "Great! Let's draw the regression line of the model according to the equation:\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\beta_0 + \\beta_1 \\cdot x_1\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lyKLrAW6o4e"
      },
      "source": [
        "param_names = ['momentum (MeV)', 'dE/dx node 1']\n",
        "plot_regression(X_train, y_train, reg, param_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1MhHiBb6o4e"
      },
      "source": [
        "As shown before, the input variable of our dataset does not follow a linear distribution, meaning that applying a linear regression model to predict the momentum from our data is not ideal. To try solve this problem, polynomial regression arises, which is, to all intents and purposes, a linear regression, only that its normal form has been extended to accommodate higher-order polynomials. In other words, it consists of fitting a model that follows the form:\n",
        "\n",
        "$$\\hat{y} = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\ldots + \\beta_g x^g$$\n",
        "\n",
        "that is, we transform the function to a polynomial of degree $g$.\n",
        "\n",
        "To do this in `sklearn` we have the preprocessing tool `preprocessing.PolynomialFeatures` which allows us to transform any data set into a new data set with polynomial and interrelated features. Since our dataset has only one feature $x_1$, if we want a polynomial of degree 3, `PolynomialFeatures` will extend the features in our set resulting in $x_1$, $x_1^2$, $x_1^3$, plus the bias. Note the exponential growth of the data set for high-degree polynomials:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6j3OOIqY6o4f"
      },
      "source": [
        "X_poly = PolynomialFeatures(degree=3, include_bias=True).fit_transform(X_stan)\n",
        "\n",
        "# we divide again into train and test\n",
        "X_poly_train, X_poly_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.4, random_state=7) # 60% training and 40% test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OMAX1Ju6o4f"
      },
      "source": [
        "Let's run the linear regression again and plot the resulted line:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKOdisV26o4f"
      },
      "source": [
        "reg = LinearRegression().fit(X_poly_train, y_train)\n",
        "\n",
        "param_names = ['momentum (MeV)', 'dE/dx node 1']\n",
        "plot_regression(X_train, y_train, reg, param_names, degree=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6psgIZO6o4f"
      },
      "source": [
        "The results are still not ideal. It kind of makes sense taking into account our dataset is underused (we are using only one input variable)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFUFjUW36o4g"
      },
      "source": [
        "A more robust but straightforward way of making the input data interpretable for the algorithm is to keep the information of only a few nodes of each track. Our preprocessing is illustrated in the following figure (there are many combinations, we are showing just one practical example here):\n",
        "\n",
        "<div>\n",
        "<img src=\"https://raw.githubusercontent.com/saulam/neutrinoml/main/reg.png\" width=\"500\"/>\n",
        "</div>\n",
        "\n",
        "where we keep the dE/dx of the first 3 and last 5 nodes of each track, along with their 4 global parameters, building up an array of size 12. For events where the track has less than 8 nodes (first 3 + last 5 nodes), we simply fill the empty positions of the array with -1s.\n",
        "\n",
        "To sum up, with this preprocessing, we should end up having our input dataset **X**, consisting of 59,578 vectors of size 12 each (a 59,578x12 matrix). The values to estimate, **y**, are the momentums of each event. Besides, it's always recommended to shuffle the training examples to prevents any bias during the training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4NZa2hZ6o4g"
      },
      "source": [
        "X = np.zeros(shape=(len(df),12), dtype=np.float32) # array of size (n_event, 12)\n",
        "y = np.zeros(shape=(len(df),), dtype=np.float32)   # array of size (n_event,)\n",
        "X.fill(-1) # filled with -1s\n",
        "\n",
        "# fill dataset\n",
        "for event_n, event in df.iterrows():\n",
        "    \n",
        "    NodeOrder = event['NodeOrder']\n",
        "    Nodededx = event['Nodededx'][NodeOrder]\n",
        "    \n",
        "    # retrieve up to the first 3 nodes\n",
        "    nfirstnodes = min(Nodededx.shape[0], 3)\n",
        "    X[event_n,:nfirstnodes] = Nodededx[:nfirstnodes]\n",
        "    \n",
        "    if Nodededx.shape[0]>nfirstnodes:\n",
        "        # retrieve up to the last 5 nodes\n",
        "        nlastnodes = min(Nodededx.shape[0]-3, 5)\n",
        "        X[event_n,nfirstnodes:nfirstnodes+nlastnodes] = Nodededx[-nlastnodes:]\n",
        "\n",
        "    # global parameters\n",
        "    X[event_n,-4] = event['TrkLen']\n",
        "    X[event_n,-3] = event['TrkEDepo']\n",
        "    X[event_n,-2] = event['TrkDir1']\n",
        "    X[event_n,-1] = event['TrkDir2']\n",
        "    \n",
        "    # momentum label\n",
        "    y[event_n] = event['TrueMomentum']\n",
        "\n",
        "# standardize the dataset (mean=0, std=1)\n",
        "X_stan = scale(X) \n",
        "\n",
        "# we divide again into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_stan, y, test_size=0.4, random_state=7) # 60% training and 40% test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yW7uwOe06o4g"
      },
      "source": [
        "In order to understand the training data, it's always good to visualise first. A good way of doing it could be creating a scatter plot of each of our 12 variables against the momentum:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYK6E2xP6o4h"
      },
      "source": [
        "param_names = ['momentum (MeV)', 'dE/dx node 1', 'dE/dx node 2', 'dE/dx node 3', 'dE/dx node n-4',\\\n",
        "               'dE/dx node n-3', 'dE/dx node n-2', 'dE/dx node n-1', 'dE/dx node n', 'TrkLen',\\\n",
        "               'TrkEDepo', 'TrkDir1', 'TrkDir2']\n",
        "plot_parameters(X, y, param_names, y_names=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPpZ29Zz6o4h"
      },
      "source": [
        "As shown above, none of the 12 variables seems to follow a linear distribution. However, let's give it a try to linear regression again:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XU9MiJR6o4h"
      },
      "source": [
        "reg = LinearRegression().fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzLWAugS6o4h"
      },
      "source": [
        "The model learnt 12 coefficients ($\\beta_1, \\beta_2, ..., \\beta_{12}$), one for each input parameter, plus the bias ($\\beta_0$):\n",
        "$$\n",
        "Y = \\beta_0 + \\beta_1 \\cdot X_1 + \\beta_2 \\cdot X_2 + \\cdots + \\beta_{12} \\cdot X_{12} + \\epsilon.\n",
        "$$\n",
        "We can print the values of the above coefficients:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRyEaOVt6o4i"
      },
      "source": [
        "print(\"B0 (bias): {:2.5}\".format(float(reg.intercept_)))\n",
        "for i, coef in enumerate(reg.coef_):\n",
        "    print(\"B{} (\\\"{}\\\"): {:2.4}\".format(i+1, param_names[i+1],coef))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIyZiyVw6o4i"
      },
      "source": [
        "Now, it's time to run our regression model on the test set. The function `plot_distributions` shows some results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1Rp7-XB6o4i"
      },
      "source": [
        "y_pred = reg.predict(X_test)\n",
        "plot_distributions(X_test, y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGjAeV5G6o4i"
      },
      "source": [
        "The regression model has clearly learnt something sensible. However, it looks like the distribution is slightly shifted to the left; thus, the difference between the original and predicted values is not symmetrical."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBXN88ja6o4j"
      },
      "source": [
        "As shown before, the input variables of our dataset do not follow a linear distribution, meaning that applying a linear regression model to predict the momentum from our data is not ideal in theory. To solve this problem, polynomial regression arises, which is, to all intents and purposes, a linear regression, only that its normal form has been extended to accommodate higher-order polynomials. In other words, it consists of fitting a model that follows the form:\n",
        "\n",
        "$$\\hat{y} = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\ldots + \\beta_g x^g$$\n",
        "\n",
        "that is, we transform the function to a polynomial of degree $g$.\n",
        "\n",
        "To do this in `sklearn` we have the preprocessing tool `preprocessing.PolynomialFeatures` which allows us to transform any data set into a new data set with polynomial and interrelated features. For example, if our data set has only two features $x_1$ and $x_2$ and we want a polynomial of degree 3, `PolynomialFeatures` will extend the features in our set resulting in $x_1$, $x_2$, $x_1^2$, $x_2^2$, $x_1 \\cdot x_2$, $x_1^3$, $x_2^3$, $x_1^2 \\cdot x_2$ and $x_1 \\cdot x_2^2$. Note the exponential growth of the data set for high-degree polynomials."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s008g6Ul6o4j"
      },
      "source": [
        "X_poly = PolynomialFeatures(degree=3, include_bias=True).fit_transform(X_stan)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96X_5uvv6o4j"
      },
      "source": [
        "We split the dataset again into training and test sets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69_TV5MT6o4k"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.4, random_state=7) # 60% training and 40% test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVrUNel66o4k"
      },
      "source": [
        "And run the linear regression again on the transformed data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvWjJ5Il6o4k"
      },
      "source": [
        "reg_poly = LinearRegression().fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsFlKTO46o4k"
      },
      "source": [
        "Let's plot the results again:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lqs4ZOc6o4l"
      },
      "source": [
        "y_pred = reg_poly.predict(X_test)\n",
        "\n",
        "plot_distributions(X_test, y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WxYUdCn6o4l"
      },
      "source": [
        "The results look more symmetrical now! The predicted distribution is not that wider, and we reduced the mean and the standard deviation of the difference between the original and the predicted values. \n",
        "\n",
        "Now it's your turn to try to beat these results! You may play with the dataset generation (using a different configuration of variables), preprocess the data differently, use another degree for the polynomial, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5Kw9AJrlxxZ"
      },
      "source": [
        "##Homework"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0Ihayi8l03V"
      },
      "source": [
        "It's your time to beat the results above!\n",
        "\n",
        "You could try to generate a new dataset based on your physics knowledge (i.e., influence the feature selection), or squeeze the regression algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJ6tRirylvjJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
