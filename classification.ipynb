{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "colab": {
      "name": "classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVjl-kju6o4S"
      },
      "source": [
        "# Particle identification\n",
        "\n",
        "This assignment aims to learn how to define and run a classification model for particle identification of neutrino events. Classification models are the most common method in machine learning. The goal is to predict a label for each input example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UG5VHLK7-iqp"
      },
      "source": [
        "##Prerequisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHMkB-apwSJy"
      },
      "source": [
        "Let's start with downloading the dataset, as well as loading the needed Python packages and modules:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1HmRISt6o4V"
      },
      "source": [
        "!wget \"https://raw.githubusercontent.com/saulam/neutrinoml/main/modules.py\"\n",
        "!wget \"https://raw.githubusercontent.com/saulam/neutrinoml/main/df_pgun_teaching.p\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pydotplus\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import scale, PolynomialFeatures\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import export_graphviz\n",
        "from sklearn.externals.six import StringIO  \n",
        "from modules import *\n",
        "from IPython.display import Image  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVHwIwSOvz3n"
      },
      "source": [
        "##Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sc7lPCGT6o4W"
      },
      "source": [
        "We can now load the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEgXr0CV6o4W"
      },
      "source": [
        "# read dataframe\n",
        "df = pd.read_pickle('df_pgun_teaching.p')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSBVlOV-6o4W"
      },
      "source": [
        "We may have a look at the dataset. It consists of 59,578 particle gun events with the following attributes:\n",
        "\n",
        "- **TruePID**: PDG code for particle identification (PID); 2212 (proton), 13 (muon), 211 (pion).\n",
        "- **TrueMomentum**: momentum in MeV.\n",
        "- **NNodes**: number of nodes of the event (3D spatial points).\n",
        "- **NodeOrder**: order of the nodes within the event.\n",
        "- **NodePosX**: array with the coordinates of the nodes along the X-axis (in mm).\n",
        "- **NodePosY**: array with the coordinates of the nodes along the Y-axis (in mm).\n",
        "- **NodePosZ**: array with the coordinates of the nodes along the Z-axis (in mm).\n",
        "- **NodeT**: array with the timestamps of the nodes (in ms).\n",
        "- **Nodededx**: array with energy deposits of the nodes (dE/dx).\n",
        "- **TrkLen**: length of the track (in mm).\n",
        "- **TrkEDepo**: total track energy deposition (in arbitrary unit).\n",
        "- **TrkDir1**: track direction, polar angle (in degrees).\n",
        "- **TrkDir2**: track direction, azimuth angle (in degrees).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9FuASKM6o4X"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RghPKyXA04rQ"
      },
      "source": [
        "And check the correlations of the variables (please notice that the node features are not included since each even has a different length):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ag5QkbJKMsVm"
      },
      "source": [
        "df.corr()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrzNwzscqd4w"
      },
      "source": [
        "The 3D spatial points of the events are usually stored in the form of hits or nodes. We chose the latter for our dataset. A hit corresponds with a cube with real energy deposition (there are usually many hits across the track signature), whilst a node corresponds with a fitted position after performing the track reconstruction.\n",
        "\n",
        "<div>\n",
        "<img src=\"https://raw.githubusercontent.com/saulam/neutrinoml/main/hit.png\" width=\"400\"/>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
        "<img src=\"https://raw.githubusercontent.com/saulam/neutrinoml/main/node.png\" width=\"400\"/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5qxEri_6o4X"
      },
      "source": [
        "We may also have a look at the events by plotting the nodes within the detector space. By default, we're looking at the first event (event 0), but we can display more events by playing with the variable `event_number`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8c9eBj3T6o4Y"
      },
      "source": [
        "event_number = 0\n",
        "plot_event(df, event_number)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g70lBcA36o4Z"
      },
      "source": [
        "Regardless of the type of data we use and the algorithm chosen, it is essential to perform a **preprocessing** of the data, which allows us to prepare the data to make it understandable for the machine-learning algorithm.\n",
        "\n",
        "As explained before, the goal is to learn to predict a label **y** from a fixed-size vector of features **X**. However, the input data is in 3D, and every event (track) has a different size. Thus, a simple way of doing it is to use two of the features to start with: `TrkLen` and `TrkEDepo`. Please, notice that in order to have a binary classification problem, we are encoding the PID code from protons (2212) and muons (13) into 0 and 1 (ignoring pions), respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yfqZYjY6o4Z"
      },
      "source": [
        "X = np.zeros(shape=(len(df),2), dtype=np.float32) # array of size (n_events, 2)\n",
        "y = np.zeros(shape=(len(df),), dtype=np.float32)  # array of size (n_events,)\n",
        "\n",
        "# fill dataset\n",
        "for event_n, event in df.iterrows():\n",
        "    \n",
        "    pid_label = event['TruePID']\n",
        "    \n",
        "    # store only protons and muons\n",
        "    if pid_label==2212 or pid_label==13:\n",
        "        # retrieve the first node\n",
        "        X[event_n, 0] = event['TrkLen']\n",
        "        X[event_n, 1] = event['TrkEDepo']\n",
        "\n",
        "        # PID label\n",
        "        if pid_label==2212:\n",
        "          pid_label=0\n",
        "        else:\n",
        "          pid_label=1\n",
        "        y[event_n] = pid_label\n",
        "\n",
        "# standardize the dataset (mean=0, std=1)\n",
        "X_stan = scale(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vlvQHUc6o4Z"
      },
      "source": [
        "In order to understand the training data, it's always good to visualise first. A good way of doing it is to create a scatter plot of one feature against the other:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDQfji9m6o4a"
      },
      "source": [
        "param_names = ['TrkLen', 'TrkEDepo']\n",
        "y_names = ['proton', 'muon']\n",
        "\n",
        "plot_params_pid(X, y, param_names, y_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-INAKi_YAYW"
      },
      "source": [
        "Good! It's easy to distinguish by eye two \"almost\" independent distributions: one for protons and the other for muons."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ee4lzVE1yP-m"
      },
      "source": [
        "## Logistic regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BX5jfc3Q6o4b"
      },
      "source": [
        "Training a machine-learning algorithm is usually not an easy task. The algorithm learns from some training data until it is ready to make predictions on unseen data. In order to test how the algorithm performs on new data, the dataset used for training is divided into two groups (sometimes is divided into three groups, but we're keeping two groups here for simplicity):\n",
        "\n",
        "- Training set: the model learns from this set only. It must be the largest set.\n",
        "- Test set: it is used to evaluate the model at the end of the training, only once it is fully trained. \n",
        "\n",
        "In this example, we keep 60% of the data for training and 40% for testing. Besides, it's always recommended to shuffle the training examples to prevents any bias during the training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74sobEvb6o4c"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_stan, y, test_size=0.4, random_state=7) # 60% training, 40% test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13zrBfUlY6LO"
      },
      "source": [
        "\n",
        "As shown in theory, despite its name, logistic regression is a binary classification algorithm based on the principles of linear regression.\n",
        "\n",
        "In logistic regression, the output of the linear prediction￼ $z = mx + b$￼ is passed to the sigmoid function $\\sigma$:\n",
        "\n",
        "$$\n",
        "\\sigma(z) = \\frac{1}{1+ e^{-z}}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S86YqgxSakhV"
      },
      "source": [
        "plot_sigmoid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSoHwFG7blyF"
      },
      "source": [
        "$$\n",
        "\\hat{y} =\n",
        "\\begin{cases}\n",
        "0 & \\text{if } \\sigma(m x + b) < 0.5 \\\\\n",
        "1 & \\text{if } \\sigma(m x + b) \\geq 0.5 \\\\\n",
        "\\end{cases}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkZsq3Qnb7va"
      },
      "source": [
        "Since the sigmoid function is bounded to the interval $(0,1)$, we can express the output of the logistic regression in probabilistic terms. The probability of belonging to each of the classes is therefore defined as:\n",
        "\n",
        "$$\n",
        "P(y|x) =\n",
        "\\begin{cases}\n",
        "\\sigma(m x+b)     & \\text{if } y = 1 \\\\\n",
        "1 - \\sigma(m x+b) & \\text{if } y = 0 \\\\\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "The logistic regression algorithm learns the parameters $m$ and the bias $b$ that satisfy the above equation. Fortunately, we don't have to perform the forward and backward propagation ourselves, and we may use the `LogisticRegression` class from `sklearn`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49VC7_dNaMZ4"
      },
      "source": [
        "log_reg = LogisticRegression(random_state=7).fit(X_train, y_train) # run the logistic regression model (random_state=7 for reproducibility)\n",
        "m, b = log_reg.coef_[0], log_reg.intercept_[0]\n",
        "print(\"m0: {}, m1: {}, b: {}\".format(m[0], m[1], b))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LZF4vVvrftl"
      },
      "source": [
        "We may now either use the logistic regression model to calculate the predictions on each event, or just calculate them analytically using the learnt parameters $m_0$, $m_1$, and $b$: \n",
        "\n",
        "$$\n",
        "\\hat{y} = \\mathbf{x}^t\\mathbf{m} + b = \n",
        "\\begin{pmatrix}\n",
        "x_{0} & x_{1}\n",
        "\\end{pmatrix}\n",
        "\\begin{pmatrix}\n",
        "m_{0} \\\\\n",
        "m_{1}\n",
        "\\end{pmatrix}+ b\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfFW3Wi5i_8Q"
      },
      "source": [
        "sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
        "event_number = 0\n",
        "prob_alg = log_reg.predict_proba(X_train[event_number].reshape(1,2))[0,1]\n",
        "prob_ana = sigmoid(np.dot(X_train[event_number].reshape(1,2),m.reshape(2,1))+b)[0,0]\n",
        "print(\"Probability from algorithm: {:1.5}, analytical probability {:1.5}\".format(prob_alg, prob_ana))\n",
        "print(\"Actual label: {}\".format(int(y_train[event_number])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJT-nZHOvQwn"
      },
      "source": [
        "We get the same probability! Since 0.0031902 < 0.5, the logistic regression model predicts the input event was a proton, which is actually correct. We can also plot the line the model learnt in order to separate protons and muons."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3jXy4qBif9R"
      },
      "source": [
        "param_names = ['TrkLen', 'TrkEDepo']\n",
        "y_names = ['proton', 'muon']\n",
        "plot_logistic_regression(log_reg, X_test, y_test, param_names, y_names) # plot the logistic regression results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rhw-cLdmyAFT"
      },
      "source": [
        "It's also usual to calculate some metrics to evaluate how good our machine-learning method performs on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9etmLMv7co3X"
      },
      "source": [
        "y_pred = log_reg.predict(X_test)\n",
        "print(\"Overall accuracy: {:2.3}\\n\".format(accuracy_score(y_test, y_pred)))\n",
        "print(\" - Proton accuracy: {:2.3}\".format(accuracy_score(y_test[y_test==0], y_pred[y_test==0])))\n",
        "print(\" - Muon accuracy: {:2.3}\\n\".format(accuracy_score(y_test[y_test==1], y_pred[y_test==1])))\n",
        "conf=confusion_matrix(y_pred, y_test)\n",
        "print_conf(conf, ['protons', 'muons'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SODIuuGgyppu"
      },
      "source": [
        "Nice! The muon accuracy might be slightly better, though. Let's increase the dimensionality of the problem!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFUFjUW36o4g"
      },
      "source": [
        "A more robust but straightforward way of making the input data interpretable for the algorithm is to keep the information of only a few nodes of each track. Our preprocessing is illustrated in the following figure (there are many combinations, we are showing just one practical example here):\n",
        "\n",
        "<div>\n",
        "<img src=\"https://raw.githubusercontent.com/saulam/neutrinoml/main/reg.png\" width=\"500\"/>\n",
        "</div>\n",
        "\n",
        "where we keep the dE/dx of the first 3 and last 5 nodes of each track, along with their 4 global parameters, building up an array of size 12. For events where the track has less than 8 nodes (first 3 + last 5 nodes), we simply fill the empty positions of the array with -1s.\n",
        "\n",
        "To sum up, with this preprocessing, we should end up having our input dataset **X**, consisting of 59,578 vectors of size 12 each (a 59,578x12 matrix). The values to estimate, **y**, are the labels of each event (proton or muon)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4NZa2hZ6o4g"
      },
      "source": [
        "X = np.zeros(shape=(len(df),12), dtype=np.float32) # array of size (n_event, 12)\n",
        "y = np.zeros(shape=(len(df),), dtype=np.float32)   # array of size (n_event,)\n",
        "X.fill(-1) # filled with -1s\n",
        "\n",
        "# fill dataset\n",
        "for event_n, event in df.iterrows():\n",
        "    pid_label = event['TruePID']\n",
        "    \n",
        "    if pid_label==2212 or pid_label==13:\n",
        "    \n",
        "      NodeOrder = event['NodeOrder']\n",
        "      Nodededx = event['Nodededx'][NodeOrder]\n",
        "\n",
        "      # retrieve up to the first 3 nodes\n",
        "      nfirstnodes = min(Nodededx.shape[0], 3)\n",
        "      X[event_n,:nfirstnodes] = Nodededx[:nfirstnodes]\n",
        "\n",
        "      if Nodededx.shape[0]>nfirstnodes:\n",
        "          # retrieve up to the last 5 nodes\n",
        "          nlastnodes = min(Nodededx.shape[0]-3, 5)\n",
        "          X[event_n,nfirstnodes:nfirstnodes+nlastnodes] = Nodededx[-nlastnodes:]\n",
        "\n",
        "      # global parameters\n",
        "      X[event_n,-4] = event['TrkLen']\n",
        "      X[event_n,-3] = event['TrkEDepo']\n",
        "      X[event_n,-2] = event['TrkDir1']\n",
        "      X[event_n,-1] = event['TrkDir2']\n",
        "\n",
        "      # PID label\n",
        "      if pid_label==2212:\n",
        "        pid_label=0\n",
        "      else:\n",
        "        pid_label=1\n",
        "      y[event_n] = pid_label\n",
        "\n",
        "# standardize the dataset (mean=0, std=1)\n",
        "X_stan = scale(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qH2A2LAzCsV"
      },
      "source": [
        "In order to understand the training data, it's always good to visualise first. A good way of doing it could be creating a histogram plot of each of our 12 features:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcFeWh3iP7E-"
      },
      "source": [
        "param_names = ['dE/dx node 1', 'dE/dx node 2', 'dE/dx node 3', 'dE/dx node n-4',\\\n",
        "               'dE/dx node n-3', 'dE/dx node n-2', 'dE/dx node n-1', 'dE/dx node n', 'TrkLen',\\\n",
        "               'TrkEDepo', 'TrkDir1', 'TrkDir2']\n",
        "y_names = [\"proton\", \"muon\"]\n",
        "plot_parameters(X, y, param_names, y_names, mode=\"classification\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YePGg59gzOwH"
      },
      "source": [
        "We split the dataset again into training and test sets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EIkyVK-vwLR"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_stan, y, test_size=0.4, random_state=7) # 60% training and 40% test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hecj03Q5zXTi"
      },
      "source": [
        "And run the logistic regression:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzrzUxPUwWn7"
      },
      "source": [
        "log_reg = LogisticRegression(random_state=7, max_iter=1000).fit(X_train, y_train)\n",
        "\n",
        "y_pred = log_reg.predict(X_test)\n",
        "print(\"Overall accuracy: {:2.3}\\n\".format(accuracy_score(y_test, y_pred)))\n",
        "print(\" - Proton accuracy: {:2.3}\".format(accuracy_score(y_test[y_test==0], y_pred[y_test==0])))\n",
        "print(\" - Muon accuracy: {:2.3}\\n\".format(accuracy_score(y_test[y_test==1], y_pred[y_test==1])))\n",
        "conf=confusion_matrix(y_pred, y_test)\n",
        "print_conf(conf, ['protons', 'muons'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjXwKJZAzz9R"
      },
      "source": [
        "The results are amazing! However, we have solved a binary classification problem, while our dataset has a third type of particles that we have ignored (pions). Although, in essence, logistic regression can only be applied to binary classification problems, it is easily extensible to solve problems with a number of classes $k>2$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McWnT5jTysUa"
      },
      "source": [
        "## Decision trees"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVOpKD9eDg78"
      },
      "source": [
        "A **decision tree** is a tree structure similar to a flowchart where an internal node represents a feature (or attribute), the branch represents a decision rule, and each leaf node represents the outcome. The top node of a **decision tree** is known as the root node. The model learns how to make the partitions based on the value of each feature. It also partitions the tree recursively, which is called *recursive partitioning*.\n",
        "\n",
        "The **decision tree** is a white-box ML algorithm. It exposes its internal decision-making logic, unlike black-box algorithms such as neural networks. This means that decision trees are **explanatory models**.\n",
        "\n",
        "It is convenient to use the `DecisionTreeClassifier` from `sklearn`, since it makes the training and testing transparent for the user. We initially configure a decision tree with a depth of 3:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5F2dYsmnzN-6"
      },
      "source": [
        "dtree = DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=7) # create decision tree\n",
        "dtree = dtree.fit(X_train,y_train) # train decision tree on train set\n",
        "y_pred = dtree.predict(X_test) # make predictions on test set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSlZi4rAhLKe"
      },
      "source": [
        "And print the performance metrics:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VEtIBrdzeF5"
      },
      "source": [
        "print(\"Overall accuracy: {:2.3}\\n\".format(accuracy_score(y_test, y_pred)))\n",
        "print(\" - Proton accuracy: {:2.3}\".format(accuracy_score(y_test[y_test==0], y_pred[y_test==0])))\n",
        "print(\" - Muon accuracy: {:2.3}\\n\".format(accuracy_score(y_test[y_test==1], y_pred[y_test==1])))\n",
        "conf=confusion_matrix(y_pred, y_test)\n",
        "y_names = ['protons', 'muons']\n",
        "print_conf(conf, y_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEPafvx2F6ep"
      },
      "source": [
        "Not bad, right? Especially if we plot the tree and try to understand how the decisions are made:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eh58febUzgWW"
      },
      "source": [
        "param_names = ['dE/dx node 1', 'dE/dx node 2', 'dE/dx node 3', 'dE/dx node n-4',\\\n",
        "               'dE/dx node n-3', 'dE/dx node n-2', 'dE/dx node n-1', 'dE/dx node n', 'TrkLen',\\\n",
        "               'TrkEDepo', 'TrkDir1', 'TrkDir2']\n",
        "dot_data = StringIO()\n",
        "export_graphviz(dtree, out_file=dot_data,  \n",
        "                filled=True, rounded=True,\n",
        "                special_characters=True,feature_names = param_names,class_names=y_names)\n",
        "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
        "#graph.write_png('diabetes.png')\n",
        "Image(graph.create_png())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOPWtKGPGd39"
      },
      "source": [
        "The learnt tree is so simple and intuitive! We may increase the complexity by playing with the `max_depth` variable of `DecisionTreeClassifier`.\n",
        "\n",
        "Last but not least, we may regenerate the dataset, but in this case considering the three types of particles (protons, muons, and pions):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lEqNMCJKHUT"
      },
      "source": [
        "X = np.zeros(shape=(len(df),12), dtype=np.float32) # array of size (n_event, 12)\n",
        "y = np.zeros(shape=(len(df),), dtype=np.float32)   # array of size (n_event,)\n",
        "X.fill(-1) # filled with -1s\n",
        "\n",
        "# fill dataset\n",
        "for event_n, event in df.iterrows():\n",
        "\n",
        "    NodeOrder = event['NodeOrder']\n",
        "    Nodededx = event['Nodededx'][NodeOrder]\n",
        "\n",
        "    # retrieve up to the first 3 nodes\n",
        "    nfirstnodes = min(Nodededx.shape[0], 3)\n",
        "    X[event_n,:nfirstnodes] = Nodededx[:nfirstnodes]\n",
        "\n",
        "    if Nodededx.shape[0]>nfirstnodes:\n",
        "        # retrieve up to the last 5 nodes\n",
        "        nlastnodes = min(Nodededx.shape[0]-3, 5)\n",
        "        X[event_n,nfirstnodes:nfirstnodes+nlastnodes] = Nodededx[-nlastnodes:]\n",
        "\n",
        "    # global parameters\n",
        "    X[event_n,-4] = event['TrkLen']\n",
        "    X[event_n,-3] = event['TrkEDepo']\n",
        "    X[event_n,-2] = event['TrkDir1']\n",
        "    X[event_n,-1] = event['TrkDir2']\n",
        "\n",
        "    # PID label\n",
        "    pid_label = event['TruePID']\n",
        "    if pid_label==2212:\n",
        "      pid_label=0 # protons\n",
        "    elif pid_label==13: \n",
        "      pid_label=1 # muons\n",
        "    else:\n",
        "      pid_label=2 # pions\n",
        "    y[event_n] = pid_label\n",
        "    y[event_n] = pid_label\n",
        "\n",
        "# standardize the dataset (mean=0, std=1)\n",
        "X_stan = scale(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_stan, y, test_size=0.4, random_state=7) # 60% training and 40% test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSHcvKkhjiej"
      },
      "source": [
        "It is always recommended to plot the histogram of each feature:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MTRTpBRionO"
      },
      "source": [
        "param_names = ['dE/dx node 1', 'dE/dx node 2', 'dE/dx node 3', 'dE/dx node n-4',\\\n",
        "               'dE/dx node n-3', 'dE/dx node n-2', 'dE/dx node n-1', 'dE/dx node n', 'TrkLen',\\\n",
        "               'TrkEDepo', 'TrkDir1', 'TrkDir2']\n",
        "y_names = [\"proton\", \"muon\", \"pions\"]\n",
        "plot_parameters(X, y, param_names, y_names, mode=\"classification\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RY-vW_QzNVMb"
      },
      "source": [
        "We retrain our decision tree on the new dataset and print the results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMhLtZw6KTvz"
      },
      "source": [
        "dtree = DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=7) # create decision tree\n",
        "dtree = dtree.fit(X_train,y_train) # train decision tree on train set\n",
        "y_pred = dtree.predict(X_test) # make predictions on test set\n",
        "\n",
        "print(\"Overall accuracy: {:2.3}\\n\".format(accuracy_score(y_test, y_pred)))\n",
        "print(\" - Proton accuracy: {:2.3}\".format(accuracy_score(y_test[y_test==0], y_pred[y_test==0])))\n",
        "print(\" - Muon accuracy: {:2.3}\".format(accuracy_score(y_test[y_test==1], y_pred[y_test==1])))\n",
        "print(\" - Pion accuracy: {:2.3}\\n\".format(accuracy_score(y_test[y_test==2], y_pred[y_test==2])))\n",
        "conf=confusion_matrix(y_pred, y_test)\n",
        "y_names = ['protons', 'muons', 'pions']\n",
        "print_conf(conf, y_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnvVsh0-NdFn"
      },
      "source": [
        "The results are not excellent. Should the tree be deeper?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1vfmgtEIKdw"
      },
      "source": [
        "##Homework"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2BF4oRRITXC"
      },
      "source": [
        "It's your time to beat the results above!\n",
        "\n",
        "You could try to generate a new dataset based on your physics knowledge (i.e., influence the feature selection), squeeze logistic regression and decision trees, or try different algorithms:\n",
        "\n",
        "- Support Vector Machines (SVMs): https://scikit-learn.org/stable/modules/svm.html.\n",
        "- Naive Bayes: https://scikit-learn.org/stable/modules/naive_bayes.html.\n",
        "- Random Forest: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html.\n",
        "- Etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DX25P_WIovR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}